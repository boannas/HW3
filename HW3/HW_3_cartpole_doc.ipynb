{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb55919",
   "metadata": {},
   "source": [
    "# **FRA 503: Deep Reinforment Learning**\n",
    "**HW 3 Cartpole Function Approximation**\n",
    "\n",
    "Napat Aeimwiratchai 65340500020  \n",
    "Phattarawat Kadrum 65340500074"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaadf9",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1. Understand how function approximation works and how to implement it.\n",
    "\n",
    "2. Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "3. Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "4. Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "5. Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2530e1",
   "metadata": {},
   "source": [
    "# **Part 1 : Understanding the algorithm**\n",
    "\n",
    "4 function approximation-based RL algorithms:\n",
    "- Linear Q-Learning \n",
    "- Deep Q-Network (DQN)\n",
    "- MC REINFORCE algorithm\n",
    "- Actor-Critic method (A2C)\n",
    "\n",
    "Value-based / Policy based / Actor-Critic approch  \n",
    "Specify type policy it learns(stochastic or deterministic)  \n",
    "Identity the type of observation space and action space (discrete or continuous)   \n",
    "Explain how each advanced RL method balances exploration and exploitation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72ba02",
   "metadata": {},
   "source": [
    "## **Linear Q-Learning** \n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: Discrete or low dimensional continuous \n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - with probability ε => exploration\n",
    "    - with probability 1-ε => exploitaion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff27a11",
   "metadata": {},
   "source": [
    "Linear Q-Learning (with TD) **Q-Update Rule:** $$ q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_a q_t(S_{t+1}, a) - q_t(S_t, A_t) \\right] $$ \n",
    "\n",
    "**Linear Function Approximation:** $$ q(s, a) \\approx \\mathbf{w}^\\top \\phi(s, a) $$ \n",
    "\n",
    "\n",
    "**Weight Update:** $$ \\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_t + \\alpha_t \\, \\delta_t \\, \\nabla_{\\mathbf{w}} q(s, a) $$ \n",
    "\n",
    "\n",
    "Where Loss: $$ \\delta_t = R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a') - q(S_t, A_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f5278",
   "metadata": {},
   "source": [
    "## **Deep Q-Network (DQN)**\n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: High dimensional (continuous)\n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - Experience Replay: decorrelates samples\n",
    "    - Target Networks: stabilize Q-value update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040a5a7",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN) **Loss Function:** $$ L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)^2 \\right] $$ **Target Q-Value:** $$ y = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') $$ **Gradient Update:** $$ \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta) $$ Where: \n",
    "- $\\theta$: parameters of the Q-network  \n",
    "- $\\theta^-$: parameters of the target network (updated periodically)  \n",
    "- $D$: experience replay buffer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971a70c",
   "metadata": {},
   "source": [
    "## **Monte Carlo REINFORCE**\n",
    "\n",
    "- Approach Type: Policy-based\n",
    "- Policy Type: Stochastic\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "- Explore vs Exploitation:\n",
    "    - Exploration from stochastic policy (softmax or Gaussian)\n",
    "    - No need ε-greedy\n",
    "    - Monte Carlo (update at episode end) => high variance\n",
    "    - Does not bootstrap (no value function), slow learning but avoids bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22137",
   "metadata": {},
   "source": [
    "REINFORCE (Monte Carlo Policy Gradient) **Policy Gradient Update:** $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $$ **Return from timestep \\( t \\):** $$ G_t = \\sum_{k=0}^{T - t - 1} \\gamma^k R_{t + k + 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4e1c0",
   "metadata": {},
   "source": [
    "## **Actor-Critic (A2C: Advantage Actor-Critic)**\n",
    "\n",
    "- Approach Type: Actor-Critic (Hybrid)\n",
    "- Policy Type: Stochastic (Actor outputs probability distribution over actions)\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "\n",
    "- Explore vs Exploitation:\n",
    "    - Stochastic policy: Actions are sample from probability distributions \n",
    "\n",
    "- Critic (value function) as a baseline: Reduce variance of policy gradient updates (used to compute Advantage)\n",
    "- Bootstrapping & TD learning\n",
    "- A2C uses TD(0) or n-step return updates\n",
    "- Update directly from gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437b45",
   "metadata": {},
   "source": [
    "**A2C (Advantage Actor-Critic)**\n",
    "\n",
    "$$\n",
    "L^{\\text{A2C}}(\\theta) = -\\mathbb{E}_t \\left[ \\log \\pi_\\theta(a_t | s_t) \\cdot \\hat{A}_t \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = r_t + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
    "$$\n",
    "\n",
    "\n",
    "Critic value loss:\n",
    "\n",
    "$$\n",
    "L^{\\text{VF}}(\\phi) = \\mathbb{E}_t \\left[ \\left( V_\\phi(s_t) - R_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Entropy bonus (to encourage exploration):\n",
    "\n",
    "$$\n",
    "L^{\\text{ENT}}(\\theta) = \\mathbb{E}_t \\left[ \\mathcal{H} \\left[ \\pi_\\theta(\\cdot | s_t) \\right] \\right]\n",
    "$$\n",
    "\n",
    "Where \\( c_v \\) and \\( c_e \\) are the coefficients for the value loss and entropy bonus respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c73a68",
   "metadata": {},
   "source": [
    "### **Comparison Table**\n",
    "|Algorithm|Approach Type|Policy Type|Observation Space|Action Space|Exploration vs Exploitation|\n",
    "|---|---|---|---|---|---|\n",
    "|Linear Q|Value based|Deterministic (ε-greedy)|Discrete|Discrete|ε-greedy|\n",
    "|DQN|Value based|Deterministic (ε-greedy)|Continuous|Discrete|ε-greedy + Replay Buffer + Target Network|\n",
    "|MC REINFORCE|Policy based|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic Policy|\n",
    "|A2C|Actor Critic|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic policy + Estimated Advantage + baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f521fb",
   "metadata": {},
   "source": [
    "# **Part 2 : Setting up `Cart-Pole` Agent**\n",
    "\n",
    "This part include: \n",
    "1. RL Base Class\n",
    "2. Replay Buffer Class\n",
    "3. Algorithm folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55274f",
   "metadata": {},
   "source": [
    "### 1.RL Base class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa41203",
   "metadata": {},
   "source": [
    "This class include:\n",
    "\n",
    "- **Constructor `(__init__)`** to initialize the following parameters:\n",
    "\n",
    "    - **Number of actions**: The total number of discrete actions available to the agent.\n",
    "\n",
    "    - **Action range**: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "    - **Learning rate**: Determines how quickly the model updates based on new information.\n",
    "\n",
    "    - **Initial epsilon**: The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "    - **Epsilon decay rate**: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "    - **Final epsilon**: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "    - **Discount factor**: A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "    - **Buffer size**: Maximum number of experiences the buffer can hold.\n",
    "\n",
    "    - **Batch size**: Number of experiences to sample per batch.\n",
    "\n",
    "- **Core Functions**\n",
    "    - `scale_action()`: scale the discrete action in range [0,n] to [action_min, action_max] range.\n",
    "    ```python\n",
    "    scaled = self.action_range[0] + (self.action_range[1] - self.action_range[0]) * (action / (self.num_of_action - 1))\n",
    "    ```\n",
    "\n",
    "    - `decay_epsilon()`: Decreases epsilon over time by inverse exponential\n",
    "    ```python\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af12671",
   "metadata": {},
   "source": [
    "### 2. Replay Buffer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1bae6",
   "metadata": {},
   "source": [
    "A class use to store state, action, reward, next state, and termination status from each timestep in episode to use as a dataset to train neural networks. This class include:\n",
    "\n",
    "- **Constructor `(__init__)`** to initialize the following parameters:\n",
    "  \n",
    "    - **memory**: FIFO buffer to store the trajectory within a certain time window.\n",
    "  \n",
    "    - **batch_size**: Number of data samples drawn from memory to train the neural network.\n",
    "\n",
    "- **Core Functions**\n",
    "  \n",
    "    - `add()`: Add state, action, reward, next state, and termination status to the FIFO buffer. Discard the oldest data in the buffer\n",
    "    ```python\n",
    "    # Create a named tuple from collections module for 5 arguments (state, action, next_state, reward, and done)\n",
    "    Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "    ```\n",
    "    - `sample()`: Sample data from memory to use in the neural network training.\n",
    "    \n",
    " \n",
    "  Note that some algorithms may not use all of the data mentioned above to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdde5f",
   "metadata": {},
   "source": [
    "### 3. Algorithm folder\n",
    "\n",
    "This folder include:\n",
    "\n",
    "- Linear Q Learning class\n",
    "- Deep Q-Network class\n",
    "- REINFORCE class\n",
    "- A2C class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8692f20",
   "metadata": {},
   "source": [
    "Each class inherit from the `BaseAlgorithm` in `RL_base_function.py` and include:\n",
    "\n",
    "- A constructor which initializes the same variables as the class it inherits from\n",
    "\n",
    "- Superclass Initialization (super().__init__())\n",
    "\n",
    "- An update() function that updates the agent’s learnable parameters and advances the training step.\n",
    "\n",
    "- A select_action() function select the action according to current policy.\n",
    "\n",
    "- A learn() function that train the regression or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e89393",
   "metadata": {},
   "source": [
    "#### Linear Q-Learning class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76779883",
   "metadata": {},
   "source": [
    "```python\n",
    "class Linear_QN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            learning_rate: float = 0.01,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:     \n",
    "        self.episode_durations = []\n",
    "\n",
    "        super().__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            initial_epsilon=initial_epsilon,\n",
    "            epsilon_decay=epsilon_decay,\n",
    "            final_epsilon=final_epsilon,\n",
    "            discount_factor=discount_factor,\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_obs,\n",
    "        next_action: int,\n",
    "        terminated: bool\n",
    "    ):\n",
    "        obs = np.array(obs)\n",
    "        next_obs = np.array(next_obs)\n",
    "\n",
    "        # Compute current Q-value for taken action\n",
    "        current_q = np.dot(obs, self.w[:, action])\n",
    "\n",
    "        # Compute TD target: reward if terminal, otherwise Bellman update\n",
    "        if terminated:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            next_q_value = np.dot(next_obs, self.w)\n",
    "            target_q = reward + self.discount_factor * np.max(next_q_value)\n",
    "\n",
    "        # TD error (difference between target and current estimate)\n",
    "        td_error = target_q - current_q\n",
    "\n",
    "        # Weight update using gradient descent on TD error\n",
    "        self.w[:, action] += self.lr * td_error.item() * obs\n",
    "\n",
    "        # Log training error\n",
    "        self.training_error.append(td_error.item() ** 2)\n",
    "        # ====================================== #\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = np.array(state)\n",
    "        # Explore with probability ε; otherwise exploit\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.num_of_action)\n",
    "        else:\n",
    "            action = np.argmax(np.dot(state, self.w))\n",
    "        return torch.tensor([[action]], dtype=torch.int64)        # return int(action)\n",
    "    \n",
    "    def learn(self, env, max_steps):\n",
    "        state = env.reset()     # Reset environment\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps:\n",
    "            # Extract current observation\n",
    "            obs = state[0]['policy'][0].cpu().numpy()\n",
    "\n",
    "            # Select action using current policy\n",
    "            action_index = self.select_action(obs)\n",
    "\n",
    "            # Scale discrete action to environment's action range\n",
    "            scaled_action = self.scale_action(action_index)\n",
    "\n",
    "            # Execute action in the environment\n",
    "            next_state, reward, done, _, __ = env.step(scaled_action)\n",
    "\n",
    "            # Extract next observation\n",
    "            next_obs = next_state['policy'][0].cpu().numpy()\n",
    "\n",
    "            # Select next action\n",
    "            next_action_index = self.select_action(next_obs)\n",
    "\n",
    "            # Update Q-values \n",
    "            self.update(obs, action_index, reward, next_obs, next_action_index, done)\n",
    "\n",
    "            # Prepare for next iteration\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "        # Decay exploration rate ε\n",
    "        self.decay_epsilon()\n",
    "        return total_reward, step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f29943",
   "metadata": {},
   "source": [
    "#### Deep Q-Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b50b7",
   "metadata": {},
   "source": [
    "```python\n",
    "# Neural Network definition for DQN policy and target networks\n",
    "class DQN_network(nn.Module):\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(DQN_network, self).__init__()\n",
    "\n",
    "        # Define a simple feedforward neural network\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass with ReLU activations and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)   # Output Q-values for all actions\n",
    "\n",
    "class DQN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.01,\n",
    "            tau: float = 0.005,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "            buffer_size: int = 1000,\n",
    "            batch_size: int = 1,\n",
    "    ) -> None:\n",
    "        # Initialize main (policy) and target networks\n",
    "        self.policy_net = DQN_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "        self.target_net = DQN_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # Setup training components\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        self.num_of_action = num_of_action\n",
    "        self.tau = tau\n",
    "\n",
    "        # Optimizer for updating policy network weights\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "        # Experience replay parameters\n",
    "        self.episode_durations = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initialize base RL settings\n",
    "        super(DQN, self).__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            initial_epsilon=initial_epsilon,\n",
    "            epsilon_decay=epsilon_decay,\n",
    "            final_epsilon=final_epsilon,  \n",
    "            discount_factor=discount_factor,\n",
    "            buffer_size=buffer_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Selects an action using ε-greedy policy\n",
    "        sample = random.random()\n",
    "        if sample > self.epsilon:\n",
    "            # probability ε: choose random action (exploration)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state.to(self.device)).argmax(dim=1).view(1, 1)\n",
    "        else:\n",
    "            # choose action with highest Q-value from policy_net (exploitation)\n",
    "            return torch.tensor([[random.randrange(self.num_of_action)]], device=self.device, dtype=torch.long)\n",
    "\n",
    "    def calculate_loss(self, non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch):\n",
    "        # Q(s,a) from policy network\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Estimate Q(s', a') for non-terminal states using target network\n",
    "        if non_final_next_states.dim() == 1:\n",
    "            non_final_next_states = non_final_next_states.unsqueeze(0)\n",
    "    \n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute expected Q value\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch.squeeze()\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
    "\n",
    "        # Loss = Huber loss between expected and actual Q-values\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        return loss\n",
    "\n",
    "    def update_policy(self):\n",
    "        sample = self.generate_sample(self.batch_size)\n",
    "        non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch = sample\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_networks(self):\n",
    "\n",
    "        # Retrieve the state dictionaries (weights) of both networks\n",
    "        policy_state_dict = self.policy_net.state_dict()\n",
    "        target_state_dict = self.target_net.state_dict()\n",
    "        policy_items = list(policy_state_dict.items())\n",
    "        target_items = list(target_state_dict.items())\n",
    "\n",
    "        updated_state_dict = {}\n",
    "\n",
    "        for (t_key, t_param), (p_key, p_param) in zip(target_items, policy_items):\n",
    "            assert t_key == p_key, \"Mismatch in parameter keys between policy and target networks.\"\n",
    "            updated_param = self.tau * p_param + (1.0 - self.tau) * t_param\n",
    "            updated_state_dict[t_key] = updated_param\n",
    "        \n",
    "        # Load the updated weights into the target network\n",
    "        self.target_net.load_state_dict(updated_state_dict)\n",
    "\n",
    "    def learn(self, env):\n",
    "    \n",
    "        obs, _ = env.reset()\n",
    "        obs_array = obs[\"policy\"] if isinstance(obs, dict) else obs\n",
    "        state = torch.tensor(obs_array, dtype=torch.float32).to(self.device) \n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        timestep = 0\n",
    "\n",
    "        while not done:\n",
    "            # Action selection\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            # Execute action in environment\n",
    "            scaled_action = self.scale_action(action.item()) \n",
    "            next_obs, reward, terminated, truncated, _ = env.step(scaled_action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_obs_array = next_obs[\"policy\"] if isinstance(next_obs, dict) else next_obs\n",
    "            next_state = torch.tensor(next_obs_array, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Prepare data for memory\n",
    "            reward = torch.tensor([reward], dtype=torch.float32).to(self.device)\n",
    "            done_tensor = torch.tensor([done], dtype=torch.bool).to(self.device)\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            self.memory.add(state, action, reward, next_state, done_tensor)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward.item()\n",
    "\n",
    "            # Train policy network on sampled minibatch\n",
    "            td_loss = self.update_policy()\n",
    "\n",
    "            # Update target network via Polyak averaging\n",
    "            self.update_target_networks()\n",
    "\n",
    "            timestep += 1\n",
    "            if done:\n",
    "                # Update exploration rate\n",
    "                self.decay_epsilon()\n",
    "        return total_reward, timestep, td_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042536f",
   "metadata": {},
   "source": [
    "#### MC REINFOCE class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfd1b2",
   "metadata": {},
   "source": [
    "```python\n",
    "# Neural network for policy π(a|s) with softmax output\n",
    "class MC_REINFORCE_network(nn.Module):\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(MC_REINFORCE_network, self).__init__()\n",
    "\n",
    "        # Feedforward network with dropout and softmax over discrete actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_size),  # 0\n",
    "            nn.ReLU(),                               # 1\n",
    "            nn.Dropout(dropout),                     # 2\n",
    "            nn.Linear(hidden_size, n_actions),       # 3\n",
    "            nn.Softmax(dim=-1)                       # 4\n",
    "        )\n",
    "        # Xavier initialization for better convergence\n",
    "        for m in self.model:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sequential forward pass through the policy network\n",
    "        x = self.model[0](x)\n",
    "        x = self.model[1](x)\n",
    "        x = self.model[2](x)\n",
    "        x = self.model[3](x)\n",
    "        x = self.model[4](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MC_REINFORCE(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "        # Policy network and optimizer\n",
    "        self.LR = learning_rate\n",
    "        self.policy_net = MC_REINFORCE_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "\n",
    "        # Inherit base parameters\n",
    "        super(MC_REINFORCE, self).__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            discount_factor=discount_factor,\n",
    "        )\n",
    "\n",
    "    def calculate_stepwise_returns(self, rewards):\n",
    "        R = 0\n",
    "        returns = []\n",
    "\n",
    "        # Compute G_t for each t in reverse\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.discount_factor * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Normalize returns to reduce variance \n",
    "        if returns.size(0) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        return returns \n",
    "\n",
    "    def generate_trajectory(self, env):\n",
    "        state = env.reset()\n",
    "        state = state[\"policy\"] \n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "\n",
    "        log_prob_actions = []\n",
    "        rewards = []\n",
    "        trajectory = []\n",
    "        done = False\n",
    "        episode_return = 0.0\n",
    "        timestep = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Compute action probabilities from current state\n",
    "            probs = self.policy_net(state)\n",
    "            m = distributions.Categorical(probs)    # Categorical distribution over actions\n",
    "\n",
    "            action = m.sample() # Sample an action\n",
    "            log_prob = m.log_prob(action)   \n",
    "\n",
    "            # Scale the discrete action for continuous environments\n",
    "            scaled_action = self.scale_action(action.item())\n",
    "\n",
    "            # Step in environment\n",
    "            next_state, reward, done, *_ = env.step(scaled_action)\n",
    "\n",
    "            # Clean up different state formats\n",
    "            if isinstance(next_state, tuple):\n",
    "                next_state = next_state[0]\n",
    "            if isinstance(next_state, dict):\n",
    "                next_state = next_state[\"policy\"] \n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            if next_state.dim() == 1:\n",
    "                next_state = next_state.unsqueeze(0)\n",
    "            \n",
    "            # Record data\n",
    "            log_prob_actions.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Prepare for next step\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            timestep += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Compute returns from rewards and stack log_probs\n",
    "        stepwise_returns = self.calculate_stepwise_returns(rewards)\n",
    "        log_prob_actions = torch.stack(log_prob_actions)\n",
    "        return episode_return, stepwise_returns, log_prob_actions, trajectory, timestep\n",
    "    \n",
    "    def calculate_loss(self, stepwise_returns, log_prob_actions):\n",
    "        return -torch.sum(stepwise_returns * log_prob_actions)\n",
    "\n",
    "    def update_policy(self, stepwise_returns, log_prob_actions):\n",
    "        loss = self.calculate_loss(stepwise_returns, log_prob_actions)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.policy_net.train()\n",
    "        episode_return, stepwise_returns, log_prob_actions, trajectory, ep_len = self.generate_trajectory(env)\n",
    "        loss = self.update_policy(stepwise_returns, log_prob_actions)\n",
    "        return episode_return, loss, trajectory, ep_len\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e18a96",
   "metadata": {},
   "source": [
    "#### A2C class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491eb63d",
   "metadata": {},
   "source": [
    "```python\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, learning_rate=1e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu_head = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Learnable log S.D. for continuous policy (Gaussian)\n",
    "        self.log_std = nn.Parameter(torch.zeros(output_dim))  # log standard deviation\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Xavier (Glorot) initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu_head(x)\n",
    "        std = torch.exp(self.log_std)   # Convert log_std to std\n",
    "        dist = Normal(mu, std)          # Gaussian policy\n",
    "        return dist\n",
    "\n",
    "# Critic Network (Value Function)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, learning_rate=0.0001):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim, output_dim)  # Value output\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "\n",
    "# Actor-Critic Agent\n",
    "class Actor_Critic(BaseAlgorithm):\n",
    "    def __init__(self, \n",
    "                 state_dim, \n",
    "                 num_of_action,\n",
    "                 learning_rate,\n",
    "                 action_range: list = [-2.5, 2.5],\n",
    "                 hidden_dim=128, \n",
    "                 gamma=0.99, \n",
    "                 device=None):\n",
    "        super().__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            discount_factor=gamma,\n",
    "        )\n",
    "\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.actor = Actor(state_dim, hidden_dim, output_dim=1, learning_rate=learning_rate).to(self.device)\n",
    "        self.critic = Critic(state_dim, hidden_dim, learning_rate=learning_rate).to(self.device)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.episode_durations = []\n",
    "        self.action_range = action_range\n",
    "        self.num_of_action = num_of_action\n",
    "\n",
    "    def scale_action_AC(self, action):\n",
    "        action_min, action_max = self.action_range\n",
    "        scaled = action_min + (action_max - action_min) * ((action + 1) / 2)  # Assuming action ∈ (-∞, ∞)\n",
    "        return torch.clamp(scaled, action_min, action_max)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        dist = self.actor(state)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        scaled_action = self.scale_action_AC(action)\n",
    "        return scaled_action, log_prob\n",
    "\n",
    "\n",
    "    def learn(self, env):\n",
    "        state_dict, _ = env.reset()\n",
    "        state = state_dict['policy'].to(self.device)\n",
    "\n",
    "        done = False\n",
    "        episode_return = 0\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        timestep = 0  \n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = self.select_action(state)\n",
    "            value = self.critic(state, action)\n",
    "\n",
    "            # Interact with environment\n",
    "            action = action.squeeze(0)\n",
    "            action_env = action.unsqueeze(0) if action.ndim == 1 else action\n",
    "            next_state_dict, reward, terminated, truncated, _ = env.step(action_env)\n",
    "            done = terminated or truncated\n",
    "            next_state = next_state_dict['policy'].to(self.device)\n",
    "\n",
    "            # Store transition data\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float32, device=self.device))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            timestep += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Monte Carlo return computation\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.cat(returns).detach()\n",
    "\n",
    "        # Compute Advantage\n",
    "        values = torch.cat(values)\n",
    "        advantage = returns - values.squeeze()\n",
    "\n",
    "        # Actor loss\n",
    "        actor_loss = -(torch.stack(log_probs) * advantage.detach()).mean()\n",
    "\n",
    "        # Critic loss\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        # Update actor\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        # Update critic\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "        return episode_return, actor_loss.item(), critic_loss.item(), len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808eaf8",
   "metadata": {},
   "source": [
    "# **Part 3 : Trainning & Playing to stabilize `Cart-Pole` Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45553b3f",
   "metadata": {},
   "source": [
    "We design experiments by changing the hyperparameters of each algorithm and training them to observe the results. Each experiment train by changing each parameter in **param** individually, while keeping all other parameters set to **defaults** values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc8ebd",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb2e63",
   "metadata": {},
   "source": [
    "In training part we create loop within n_episode for train agent, we use tensorboard to logging data from agent's parameters. Below is the base training code for all the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3172f",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    # hyperparameters\n",
    "    num_of_action = xx\n",
    "    action_range = [-xx, xx]  \n",
    "    learning_rate = xx\n",
    "    hidden_dim = xx\n",
    "    n_episodes = xx\n",
    "    initial_epsilon = xx\n",
    "    epsilon_decay = xx\n",
    "    final_epsilon = xx\n",
    "    discount = xx\n",
    "    buffer_size = xx\n",
    "    batch_size = xx\n",
    "\n",
    "    # Setup matplotlib \n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "    plt.ion()\n",
    "\n",
    "    # if GPU is to be used\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "\n",
    "    task_name = str(args_cli.task).split('-')[0]  # Stabilize, SwingUp\n",
    "    Algorithm_name = \"Linear_Q\"\n",
    "\n",
    "    # Depend on which algorithm class we use\n",
    "    agent = Algo_xxx(\n",
    "        device = device,\n",
    "        num_of_action = num_of_action,\n",
    "        action_range = action_range,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_dim = hidden_dim,\n",
    "        initial_epsilon = initial_epsilon,\n",
    "        epsilon_decay = epsilon_decay,\n",
    "        final_epsilon = final_epsilon,\n",
    "        discount_factor = discount,\n",
    "        buffer_size = buffer_size,\n",
    "        batch_size = batch_size,\n",
    "    )\n",
    "\n",
    "    # reset environment\n",
    "    obs, _ = env.reset()\n",
    "    timestep = 0\n",
    "\n",
    "    # tensor board logging\n",
    "    log_dir = os.path.join(\"runs\", f\"{task_name}_{Algorithm_name}_{num_of_action}_{action_range[1]}\")\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    # simulate environment\n",
    "    while simulation_app.is_running():\n",
    "\n",
    "        # run everything in inference mode\n",
    "        with torch.inference_mode():\n",
    "            for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "                # Agent learning \n",
    "                rewards, episode_len = agent.learn(env)\n",
    "\n",
    "                # logging data\n",
    "                writer.add_scalar(\"Reward/Episode\", rewards, episode)\n",
    "                writer.add_scalar(\"Policy/Epsilon\", agent.epsilon, episode)\n",
    "                writer.add_scalar(\"Episode/Length\", episode_len, episode)\n",
    "                episode_rewards.append(rewards.item())\n",
    "                episode_lengths.append(episode_len)\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "\n",
    "                    # logging average over 100 episodes\n",
    "                    avg_rew_100 = np.mean(episode_rewards[-100:])\n",
    "                    avg_ep_len_100 = np.mean(episode_rewards[-100:])\n",
    "                    writer.add_scalar(\"Reward/Avg_Reward_100\", avg_rew_100, episode)\n",
    "                    writer.add_scalar(\"Episode/Avg_Ep_len_100\", avg_ep_len_100, episode)\n",
    "\n",
    "                    print(\"epsilon : \", agent.epsilon)\n",
    "                    print(\"reward : \", avg_rew_100)\n",
    "\n",
    "                    # Save Q-Learning agent\n",
    "                    w_file = f\"{Algorithm_name}_{episode}_{num_of_action}_{action_range[1]}.json\"\n",
    "                    full_path = os.path.join(f\"w/{task_name}\", Algorithm_name)\n",
    "                    agent.save_w(full_path, w_file)\n",
    "        \n",
    "        print('Complete')\n",
    "        agent.plot_durations(show_result=True)\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "            \n",
    "        if args_cli.video:\n",
    "            timestep += 1\n",
    "            # Exit the play loop after recording one video\n",
    "            if timestep == args_cli.video_length:\n",
    "                break\n",
    "        break\n",
    "    # close the simulator\n",
    "    env.close()\n",
    "    writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbba61",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d122a",
   "metadata": {},
   "source": [
    "In the below play code is the base code for `play.py` for all algorithm, this code need to load model from `training.py` that save weight of the model. We code to collect state and video of the highest alive time for range of n_episode that we will analyze and visualize later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138381bd",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    # hyperparameter\n",
    "    num_of_action = xx\n",
    "    action_range = [-xx, xx]  \n",
    "    learning_rate = xx\n",
    "    hidden_dim = xx\n",
    "    n_episodes = xx\n",
    "    initial_epsilon = xx\n",
    "    epsilon_decay = xx\n",
    "    final_epsilon = xx\n",
    "    discount = xx\n",
    "    buffer_size = xx\n",
    "    batch_size = xx\n",
    "\n",
    "    # set up matplotlib\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "    plt.ion()\n",
    "\n",
    "    # if GPU is to be used\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "\n",
    "    # Depend on which algorithm class we use\n",
    "    agent = Algo_xxx(\n",
    "        device = device,\n",
    "        num_of_action = num_of_action,\n",
    "        action_range = action_range,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_dim = hidden_dim,\n",
    "        initial_epsilon = initial_epsilon,\n",
    "        epsilon_decay = epsilon_decay,\n",
    "        final_epsilon = final_epsilon,\n",
    "        discount_factor = discount,\n",
    "        buffer_size = buffer_size,\n",
    "        batch_size = batch_size,\n",
    "    )\n",
    "\n",
    "    task_name = str(args_cli.task).split('-')[0]  # Stabilize, SwingUp\n",
    "\n",
    "    # Select model weight for each algorithm\n",
    "    q_value_file = \"xxxx.json\"    \n",
    "    full_path = os.path.join(f\"w/{task_name}\", Algorithm_name)\n",
    "    agent.load_w(full_path, q_value_file)\n",
    "\n",
    "    # reset environment\n",
    "    obs, _ = env.reset()\n",
    "    timestep = 0\n",
    "    # simulate environment\n",
    "    while simulation_app.is_running():\n",
    "        longest_episode_log = []\n",
    "        max_steps = 0\n",
    "        best_return = 0.0\n",
    "        # run everything in inference mode\n",
    "        with torch.inference_mode():\n",
    "            for episode in range(n_episodes):\n",
    "                obs, _ = env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                steps = 0\n",
    "                episode_log = []\n",
    "                episode_frames = []\n",
    "\n",
    "                obs = obs['policy'][0].cpu().numpy()\n",
    "                while not done:\n",
    "\n",
    "                    # Agent selects action\n",
    "                    action = agent.select_action(obs)\n",
    "\n",
    "                    # Environment step\n",
    "                    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    total_reward += reward\n",
    "                    steps += 1\n",
    "\n",
    "                    # Log state\n",
    "                    tmp = next_obs.cpu().numpy().flatten().tolist() if isinstance(next_obs, torch.Tensor) else np.array(next_obs).flatten().tolist()\n",
    "                    episode_log.append(tmp[0]['policy'].cpu().numpy()[0])\n",
    "                    \n",
    "                    # Update observation\n",
    "                    obs = next_obs['policy'][0].cpu().numpy()\n",
    "\n",
    "                    # Capture frame\n",
    "                    if args_cli.video:\n",
    "                        frame = env.render()\n",
    "                        episode_frames.append(frame)\n",
    "\n",
    "                if steps > max_steps:\n",
    "                    max_steps = steps\n",
    "                    longest_episode_log = episode_log\n",
    "                    longest_episode_frames = episode_frames\n",
    "                    best_return = total_reward\n",
    "\n",
    "        # Save state log to CSV\n",
    "        with open('Linear_Q_longest_episode_5k.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(longest_episode_log)\n",
    "\n",
    "        print(f\"\\nLongest episode: {max_steps} steps | Return: {best_return.item():.4f}\")\n",
    "        print(\"Saved as 'Linear_Q_longest_episode.csv'\")\n",
    "\n",
    "        # Save video\n",
    "        if args_cli.video and longest_episode_frames:\n",
    "            video_path = Path(\"Linear_Q_longest_episode_5k.mp4\")\n",
    "            imageio.mimsave(video_path, longest_episode_frames, fps=100)\n",
    "            print(f\"Saved video as '{video_path}'\")\n",
    "\n",
    "        break  # Exit after evaluation\n",
    "\n",
    "    # close the simulator\n",
    "    env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc4d75",
   "metadata": {},
   "source": [
    "## Plot state "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee36b86",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "csv_files= [\n",
    "    'DQN_longest_episode_hi256.csv',\n",
    "    'Linear_Q_longest_episode.csv',\n",
    "    'MC_longest_episode.csv',\n",
    "    'AC_longest_episode_ar_10_10.csv'\n",
    "]\n",
    "columns = [\"Pos:cart\", \"Pos:pole\", \"Vel:cart\", \"Vel:pole\"]\n",
    "\n",
    "def extract_label(filename):\n",
    "    \"\"\"Extract algorithm name from filename prefix.\"\"\"\n",
    "    match = re.match(r'([A-Za-z_]+)_longest_episode', filename)\n",
    "    if match:\n",
    "        return match.group(1).replace(\"_\", \"\")\n",
    "    return \"default\"\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file, header=None, names=columns)\n",
    "        label = extract_label(csv_file)\n",
    "        plt.plot(df[column], label=label, linewidth=1)\n",
    "\n",
    "    plt.title(f'{column}', fontsize=10)\n",
    "    plt.xlabel('Time Step', fontsize=8)\n",
    "    plt.ylabel('Value', fontsize=8)\n",
    "    plt.legend(fontsize='x-small')\n",
    "plt.savefig(\"All.png\", dpi=300)  \n",
    "\n",
    "plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c038e1",
   "metadata": {},
   "source": [
    "## Linear Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94b088",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 5,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'n_episodes': 3000,\n",
    "    'initial_epsilon': 1.0,\n",
    "    'epsilon_decay': 0.998,\n",
    "    'final_epsilon': 0.001,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df50f9",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"discount\": [0.8],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e70d57",
   "metadata": {},
   "source": [
    "**3000 episodes train**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ea582",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2dba30",
   "metadata": {},
   "source": [
    "Highest reward from 3000 episodes is default with adjust *`lower action range(orange)`* from default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d947710",
   "metadata": {},
   "source": [
    "**Increase train to 5000 episodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f890",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward_5000ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815d9bd",
   "metadata": {},
   "source": [
    "Highest reward from 5000 episodes is default with adjust *`lower action range(dark blue)`* from default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21446a0d",
   "metadata": {},
   "source": [
    "From the reward graph, we select the model trained by changing *`action_range`*, as it give the highest overall reward across both 3000(orange line action_range = [-10,10]) and 5000 episodes(dark blue line action_range = [-10,10])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc7a64",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/Linear_Q_longest_episode.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>action range (10,10) from 3000 episode</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/Linear_Q_longest_episode_5k.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>action range (10,10) from 5000 episode</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456ea32",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4978072",
   "metadata": {},
   "source": [
    "This image sample **agent play on 1 episode before terminated**, it **show state(pos, velo) of cart and pole** that label on the title. Both 2 parameter is for 3,000(default) and 5,000(5k) episodes. On default parameter cart pole can stabilize pole but terminnate by sliding out of the boundary but in 5k doesn't has the same result that can not hold the stability task(terminate by pole out of coundary). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124870d",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/LQ_raw.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>LQ raw rewards during training</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/LQ_avg.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>LQ average rewards during training</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec550e20",
   "metadata": {},
   "source": [
    "This image show the variance of reward per episode that red line(5k eps) has higher variance than the green line(3k eps) although the average reward of 5k episodes higher than 3k episodes but has higher variance, so, in conclusion we think **3000 episode with action range [-10, 10] is better!**\n",
    "\n",
    "**The best adjusting for Linear Q is *lower action range(-10, 10)* from the default(-20, 20)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f2fa4",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825d123",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'tau': 0.005,\n",
    "    'dropout': 0.1,\n",
    "    'initial_epsilon': 1.00,\n",
    "    'epsilon_decay': 0.998,\n",
    "    'final_epsilon': 0.01,\n",
    "    'discount': 0.9,\n",
    "    'buffer_size': 10000,\n",
    "    'batch_size': 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf4f31",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "    \"tau\": [0.001, 0.01],\n",
    "    \"dropout\": [0.2],\n",
    "    \"buffer_size\": [50000],\n",
    "    \"discount\": [0.8]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4b172",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/DQN_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ccbd1",
   "metadata": {},
   "source": [
    "Since the reward graph makes it difficult to see the **different of performance from the average reward between each adjust**, we ran all the models to determine which one give the longest episode length, as shown in the graph below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cb7b5",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/DQN.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dedf2",
   "metadata": {},
   "source": [
    "Though mostly of DQN adjust can stabilize the pole but we need to find the adjust that give the highest rewards, then we select the model that train by changing *`hidden_dim`* as it give longest step also the highest rewards too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa85311",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <video width=\"800\" controls>\n",
    "    <source src=\"result/DQN_longest_episode_hi256.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821f4a8",
   "metadata": {},
   "source": [
    "\n",
    "**The best for DQN is default parameter with *higher hidden dimension(256)* than default(128)**\n",
    "\n",
    "Although in this model can alive the longest time, it terminate by sliding out of boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab328d8b",
   "metadata": {},
   "source": [
    "## Monte Carlo REINFORCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194f961",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'n_observations': 4,\n",
    "    'dropout': 0.1,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee7ca",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.0005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"discount\": [0.8],\n",
    "    \"dropout\": [0.2],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df80978",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa87035",
   "metadata": {},
   "source": [
    "From the reward graph, we select the model trained by changing *`discount_factor(Pink line)`* that giving the highest average reward at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd95de0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <video width=\"800\" controls>\n",
    "    <source src=\"result/MC_longest_episode.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92bc27",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10f165",
   "metadata": {},
   "source": [
    "**The best for MC REINFORCE is default parameter with *lower discount factor(0.8)* than default(0.9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12e73",
   "metadata": {},
   "source": [
    "## Actor-Critic (A2C: Advantage Actor-Critic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ee0f0",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'n_observations': 4,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859def77",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"discount\": [0.8],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a435297",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/AC_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>LQ raw rewards during training</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/AC_loss.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>LQ average rewards during training</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3f049",
   "metadata": {},
   "source": [
    "As seen in the left graph, the *default(pink)* parameters and *action_range(black)* model are the highest reward and have similar rewards, so we selected both models to run further evaluations.  \n",
    "\n",
    "But in the right image, some adjust(higher hidden dimension - orange) has higher loss from the actor that prone the overfitting problem(reward decrease to zero) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52148",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_default.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>default parameter</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_ar_10_10.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>changing action_range</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7ff66",
   "metadata": {},
   "source": [
    "Both Actor critic adjust **can alive with the full timestep(1000 steps or 10 seconds) but which one is better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a36442",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/blackpink.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fff84b",
   "metadata": {},
   "source": [
    "In this image show how **stable of rewards across the episode**, in action range(black) has higher reward bound than default(pink) and also more stable reward too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f35f2",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/AC.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8a6d4",
   "metadata": {},
   "source": [
    "So, we plot the position and velocity of both to visualize, both can stabilize pole also stabilize cart in the boundary but we can see the **rate of change** of position and velocity that **lower action range(orange)** has **more stable and lower rate of change** with compare to default(blue)\n",
    "\n",
    "**The best for AC is default parameter with *lower action range(-10, 10)* than default(-20, 20)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f6ee9",
   "metadata": {},
   "source": [
    "# **Part 4 : Evaluate `Cart-Pole` Agent performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3613d40",
   "metadata": {},
   "source": [
    "- Learning efficiency (how well agent learns to recieve higher rewards)\n",
    "- Deployment performance (how well the agent perform in stabilize problem)\n",
    "\n",
    "Analyze and visualize the result to determine: \n",
    "1. Which algo performs best?\n",
    "2. Why does it perform better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c77111",
   "metadata": {},
   "source": [
    "From Part 3, the *Actor-Critic (A2C: Advantage Actor-Critic)* give the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aa3cf",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_default.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>default parameter</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_ar_10_10.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>changing action_range</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9aaa8",
   "metadata": {},
   "source": [
    "`A2C` combine both value-based and policy-based\n",
    "- Actor learn policy\n",
    "- Critic learn value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e61e49",
   "metadata": {},
   "source": [
    "Compare to `Linear Q Learning` that use linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff2ad2",
   "metadata": {},
   "source": [
    "$$ q(s, a) \\approx \\mathbf{w}^\\top \\phi(s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e94a28",
   "metadata": {},
   "source": [
    "So Linear model may not be able to model the non-linear(complex) state-action space in this task, as the cart-pole dynamics are non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6e3be",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward_ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0890a1",
   "metadata": {},
   "source": [
    "Where A2C use neural-network so it can handle more complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5cf02",
   "metadata": {},
   "source": [
    "In `MC_Reinforce` it update policy gradient $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $$ where it compute return ($G_t$) from Monte-Carlo algorithm $$ G_t = \\sum_{k=0}^{T - t - 1} \\gamma^k R_{t + k + 1} $$ so it may lead to high-varaince during traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68342b0",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC_reward_ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb7b93",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/MC_lately.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>MC reward during traning</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/AC_lately.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>AC reward during traning</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352415ba",
   "metadata": {},
   "source": [
    "As can see AC has less variance in the lately episode of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b92db",
   "metadata": {},
   "source": [
    "Moreover, we think that `DQN` and `MC_RL` struggle with the local optima problem, as can see in the video where the cart can stabilize pole but cart moves slightly to the left or right before the episode terminates(cart terminate by move out of boundary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65321fcf",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/DQN_longest_episode_hi256.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>DQN</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/MC_longest_episode.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>MC_RL</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd0a96",
   "metadata": {},
   "source": [
    "This may be due to the reward function, which gives a reward simply for keeping the cart-pole alive. As a result, the agent focuses more on surviving as long as possible rather than keeping the pole upright at the center.\n",
    "\n",
    "```py\n",
    "# (1) Constant running reward\n",
    "alive = RewTerm(func=mdp.is_alive, weight=1.0)\n",
    "# (2) Failure penalty\n",
    "terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)\n",
    "# (3) Primary task: keep pole upright\n",
    "pole_pos = RewTerm(\n",
    "    func=mdp.joint_pos_target_l2,\n",
    "    weight=-1.0,\n",
    "    params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    ")\n",
    "```\n",
    "\n",
    "Now we use only 3 reward terms that gain the reward when `cart pole` alive and pole position is upright and reduce reward when it terminate, but it doesn't has term for gain reward when cart has stable position or doesn't move, so, we can add the below reward term for cart pole more stable both cart and pole\n",
    "\n",
    "```py\n",
    "# (4) Shaping tasks: lower cart velocity\n",
    "cart_vel = RewTerm(\n",
    "    func=mdp.joint_vel_l1,\n",
    "    weight=-0.01,\n",
    "    params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n",
    ")\n",
    "# (5) Shaping tasks: lower pole angular velocity\n",
    "pole_vel = RewTerm(\n",
    "    func=mdp.joint_vel_l1,\n",
    "    weight=-0.005,\n",
    "    params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328fa3e",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "For all approximation function we choose the best algorithm that can stabilize pole upright with the longest alive time including lowest variance mean every time we use this model for play stailize model should has closely reward for all random environment. \n",
    "\n",
    "#### **The best algorithm is `Advantage Actor-Critic (A2C) with lower action range` algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd3b0d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <video width=\"800\" controls>\n",
    "    <source src=\"result/AC_longest_episode_ar_10_10.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
