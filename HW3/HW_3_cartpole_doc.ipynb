{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb55919",
   "metadata": {},
   "source": [
    "# **FRA 503: Deep Reinforment Learning**\n",
    "**HW 3 Cartpole Function Approximation**\n",
    "\n",
    "Napat Aeimwiratchai 65340500020  \n",
    "Phattarawat Kadrum 65340500074"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaadf9",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1. Understand how function approximation works and how to implement it.\n",
    "\n",
    "2. Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "3. Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "4. Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "5. Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2530e1",
   "metadata": {},
   "source": [
    "# **Part 1 : Understanding the algorithm**\n",
    "\n",
    "4 function approximation-based RL algorithms:\n",
    "- Linear Q-Learning \n",
    "- Deep Q-Network (DQN)\n",
    "- MC REINFORCE algorithm\n",
    "- Actor-Critic method (xxxxx)\n",
    "\n",
    "Value-based / Policy based / Actor-Critic approch  \n",
    "Specify type policy it learns(stochastic or deterministic), identity the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72ba02",
   "metadata": {},
   "source": [
    "## Linear Q-Learning \n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: Discrete or low dimensional continuous \n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - with probability ε => exploration\n",
    "    - with probability 1-ε => exploitaion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff27a11",
   "metadata": {},
   "source": [
    "Linear Q-Learning (with TD) **Q-Update Rule:** $$ q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_a q_t(S_{t+1}, a) - q_t(S_t, A_t) \\right] $$ **Linear Function Approximation:** $$ q(s, a) \\approx \\mathbf{w}^\\top \\phi(s, a) $$ **Weight Update:** $$ \\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_t + \\alpha_t \\, \\delta_t \\, \\nabla_{\\mathbf{w}} q(s, a) $$ Where: $$ \\delta_t = R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a') - q(S_t, A_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f5278",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: High dimensional (continuous)\n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - Experience Replay: decorrelates samples\n",
    "    - Target Networks: stabilize Q-value update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040a5a7",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN) **Loss Function:** $$ L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)^2 \\right] $$ **Target Q-Value:** $$ y = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') $$ **Gradient Update:** $$ \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta) $$ Where: \n",
    "- $\\theta$: parameters of the Q-network  \n",
    "- $\\theta^-$: parameters of the target network (updated periodically)  \n",
    "- $D$: experience replay buffer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971a70c",
   "metadata": {},
   "source": [
    "## Monte Carlo REINFORCE\n",
    "\n",
    "- Approach Type: Policy-based\n",
    "- Policy Type: Stochastic\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "- Explore vs Exploitation:\n",
    "    - Exploration from stochastic policy (softmax or Gaussian)\n",
    "    - No need ε-greedy\n",
    "    - Monte Carlo (update at episode end) => high variance\n",
    "    - Does not bootstrap (no value function), slow learning but avoids bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22137",
   "metadata": {},
   "source": [
    "REINFORCE (Monte Carlo Policy Gradient) **Policy Gradient Update:** $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $$ **Return from timestep \\( t \\):** $$ G_t = \\sum_{k=0}^{T - t - 1} \\gamma^k R_{t + k + 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4e1c0",
   "metadata": {},
   "source": [
    "## Actor-Critic (A2C: Advantage Actor-Critic)\n",
    "\n",
    "- Approach Type: Actor-Critic (Hybrid)\n",
    "- Policy Type: Stochastic (Actor outputs probability distribution over actions)\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "\n",
    "- Explore vs Exploitation:\n",
    "    - Stochastic policy: Actions are sample from probability distributions \n",
    "\n",
    "- Critic (value function) as a baseline: Reduce variance of policy gradient updates (used to compute Advantage)\n",
    "- Bootstrapping & TD learning\n",
    "- A2C uses TD(0) or n-step return updates\n",
    "- Update directly from gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437b45",
   "metadata": {},
   "source": [
    "A2C (Advantage Actor-Critic)\n",
    "\n",
    "$$\n",
    "L^{\\text{A2C}}(\\theta) = -\\mathbb{E}_t \\left[ \\log \\pi_\\theta(a_t | s_t) \\cdot \\hat{A}_t \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = r_t + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
    "$$\n",
    "\n",
    "or with n-step returns:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = \\left( \\sum_{i=0}^{n-1} \\gamma^i r_{t+i} \\right) + \\gamma^n V_{\\phi}(s_{t+n}) - V_{\\phi}(s_t)\n",
    "$$\n",
    "\n",
    "Critic value loss:\n",
    "\n",
    "$$\n",
    "L^{\\text{VF}}(\\phi) = \\mathbb{E}_t \\left[ \\left( V_\\phi(s_t) - R_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Entropy bonus (to encourage exploration):\n",
    "\n",
    "$$\n",
    "L^{\\text{ENT}}(\\theta) = \\mathbb{E}_t \\left[ \\mathcal{H} \\left[ \\pi_\\theta(\\cdot | s_t) \\right] \\right]\n",
    "$$\n",
    "\n",
    "Total A2C loss:\n",
    "\n",
    "$$\n",
    "L^{\\text{total}} = L^{\\text{A2C}} + c_v L^{\\text{VF}} - c_e L^{\\text{ENT}}\n",
    "$$\n",
    "\n",
    "Where \\( c_v \\) and \\( c_e \\) are the coefficients for the value loss and entropy bonus respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c73a68",
   "metadata": {},
   "source": [
    "### **Comparison Table**\n",
    "|Algorithm|Approach Type|Policy Type|Observation Space|Action Space|Exploration vs Exploitation|\n",
    "|---|---|---|---|---|---|\n",
    "|Linear Q|Value based|Deterministic (ε-greedy)|Discrete|Discrete|ε-greedy|\n",
    "|DQN|Value based|Deterministic (ε-greedy)|Continuous|Discrete|ε-greedy + Replay Buffer + Target Network|\n",
    "|MC REINFORCE|Policy based|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic Policy|\n",
    "|A2C|Actor Critic|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic policy + Estimated Advantage + baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f521fb",
   "metadata": {},
   "source": [
    "# **Part 2 : Setting up `Cart-Pole` Agent**\n",
    "\n",
    "1. RL Base Class\n",
    "2. Replay Buffer Class\n",
    "3. Algorithm folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808eaf8",
   "metadata": {},
   "source": [
    "# **Part 3 : Trainning & Playing to stabilize `Cart-Pole` Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45553b3f",
   "metadata": {},
   "source": [
    "We design experiments by changing the hyperparameters of each algorithm and training them to observe the results. Each experiment train by changing each parameter in **param** individually, while keeping all other parameters set to **defaults** values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c038e1",
   "metadata": {},
   "source": [
    "## Linear Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94b088",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 5,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'n_episodes': 5000,\n",
    "    'initial_epsilon': 1.0,\n",
    "    'epsilon_decay': 0.998,\n",
    "    'final_epsilon': 0.001,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df50f9",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"discount\": [0.8],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ea582",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d947710",
   "metadata": {},
   "source": [
    "Increase trian to 5000 episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f890",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward_5000ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21446a0d",
   "metadata": {},
   "source": [
    "From the reward graph, we select the model trained by changing *action_range*, as it give the highest overall reward across both 3000 and 5000 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc7a64",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/Linear_Q_longest_episode.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>from 3000 episode</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/Linear_Q_longest_episode_5k.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>from 5000 episode</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456ea32",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f2fa4",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825d123",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'tau': 0.005,\n",
    "    'dropout': 0.1,\n",
    "    'initial_epsilon': 1.00,\n",
    "    'epsilon_decay': 0.998,\n",
    "    'final_epsilon': 0.01,\n",
    "    'discount': 0.9,\n",
    "    'buffer_size': 10000,\n",
    "    'batch_size': 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf4f31",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "    \"tau\": [0.001, 0.01],\n",
    "    \"dropout\": [0.2],\n",
    "    \"buffer_size\": [50000],\n",
    "    \"discount\": [0.8]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4b172",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/DQN_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ccbd1",
   "metadata": {},
   "source": [
    "Since the reward graph makes it difficult to see the different of performance between each model, we ran all the models to determine which one give the longest episode length, as shown in the graph below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cb7b5",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/DQN.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dedf2",
   "metadata": {},
   "source": [
    "And then we select the model that train by changing *hidden_dim* as it give longest step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa85311",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <video width=\"800\" controls>\n",
    "    <source src=\"result/DQN_longest_episode_hi256.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab328d8b",
   "metadata": {},
   "source": [
    "## Monte Carlo REINFORCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194f961",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'n_observations': 4,\n",
    "    'dropout': 0.1,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee7ca",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.0005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"discount\": [0.8],\n",
    "    \"dropout\": [0.2],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df80978",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa87035",
   "metadata": {},
   "source": [
    "From the reward graph, we select the model trained by changing *discount_factor*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd95de0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <video width=\"800\" controls>\n",
    "    <source src=\"result/MC_longest_episode.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92bc27",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12e73",
   "metadata": {},
   "source": [
    "## Actor-Critic (A2C: Advantage Actor-Critic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ee0f0",
   "metadata": {},
   "source": [
    "```py\n",
    "defaults = {\n",
    "    'num_of_action': 7,\n",
    "    'action_range': [-20.0, 20.0],\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 128,\n",
    "    'n_episodes': 3000,\n",
    "    'n_observations': 4,\n",
    "    # 'dropout': 0.1,\n",
    "    'discount': 0.9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859def77",
   "metadata": {},
   "source": [
    "```py\n",
    "param = {\n",
    "    \"learning_rate\": [0.005],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"discount\": [0.8],\n",
    "    \"action_range\": [[-10, 10]],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a435297",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/AC_reward.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389208a4",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/blackpink.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eeba95",
   "metadata": {},
   "source": [
    "As seen in the graph, the *default* parameters and *action_range* model have similar rewards, so we selected both models to run further evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52148",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_default.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>default parameter</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_ar_10_10.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>changing action_range</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f35f2",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/AC.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f6ee9",
   "metadata": {},
   "source": [
    "# **Part 4 : Evaluate `Cart-Pole` Agent performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3613d40",
   "metadata": {},
   "source": [
    "- Learning efficiency (how well agent learns to recieve higher rewards)\n",
    "- Deployment performance (how well the agent perform in stabilize problem)\n",
    "\n",
    "Analyze and visualize the result to determine: \n",
    "1. Which algo performs best?\n",
    "2. Why does it perform better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c77111",
   "metadata": {},
   "source": [
    "From Part 3, the *Actor-Critic (A2C: Advantage Actor-Critic)* give the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aa3cf",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_default.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>default parameter</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/AC_longest_episode_ar_10_10.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>changing action_range</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e61e49",
   "metadata": {},
   "source": [
    "Compare to `Linear Q Learning` that use linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff2ad2",
   "metadata": {},
   "source": [
    "$$ q(s, a) \\approx \\mathbf{w}^\\top \\phi(s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e94a28",
   "metadata": {},
   "source": [
    "So it may not be able to model the non-linear state-action space in this task, as the cart-pole dynamics are non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6e3be",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/LQ_reward_ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0890a1",
   "metadata": {},
   "source": [
    "Where A2C use nueral-network so it can handle more complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5cf02",
   "metadata": {},
   "source": [
    "In `MC_Reinforce` it update policy gradient $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $$ where it compute return ($G_t$) from Monte-Carlo algorithm $$ G_t = \\sum_{k=0}^{T - t - 1} \\gamma^k R_{t + k + 1} $$ so it may lead to high-varaince during traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68342b0",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"result/MC_reward_ep.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb7b93",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/MC_lately.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>MC reward during traning</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <img src=\"result/AC_lately.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "    <p>AC reward during traning</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352415ba",
   "metadata": {},
   "source": [
    "As can see AC has less variance in the lately episode of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b92db",
   "metadata": {},
   "source": [
    "Moreover, we think that `DQN` and `MC_RL` struggle with the local optima problem, as can see in the video where the cart moves slightly to the left or right before the episode terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65321fcf",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; text-align: center;\">\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/DQN_longest_episode_hi256.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>DQN</p>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"width: 48%;\">\n",
    "    <video width=\"100%\" controls>\n",
    "      <source src=\"result/MC_longest_episode.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <p>MC_RL</p>\n",
    "  </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
