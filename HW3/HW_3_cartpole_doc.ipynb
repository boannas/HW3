{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb55919",
   "metadata": {},
   "source": [
    "# **FRA 503: Deep Reinforment Learning**\n",
    "**HW 3 Cartpole Function Approximation**\n",
    "\n",
    "Napat Aeimwiratchai 65340500020  \n",
    "Phattarawat Kadrum 65340500074"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaadf9",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1. Understand how function approximation works and how to implement it.\n",
    "\n",
    "2. Understand how policy-based RL works and how to implement it.\n",
    "\n",
    "3. Understand how advanced RL algorithms balance exploration and exploitation.\n",
    "\n",
    "4. Be able to differentiate RL algorithms based on stochastic or deterministic policies, as well as value-based, policy-based, or Actor-Critic approaches.\n",
    "\n",
    "5. Gain insight into different reinforcement learning algorithms, including Linear Q-Learning, Deep Q-Network (DQN), the REINFORCE algorithm, and the Actor-Critic algorithm. Analyze their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2530e1",
   "metadata": {},
   "source": [
    "# **Part 1 : Understanding the algorithm**\n",
    "\n",
    "4 function approximation-based RL algorithms:\n",
    "- Linear Q-Learning \n",
    "- Deep Q-Network (DQN)\n",
    "- MC REINFORCE algorithm\n",
    "- Actor-Critic method (xxxxx)\n",
    "\n",
    "Value-based / Policy based / Actor-Critic approch  \n",
    "Specify type policy it learns(stochastic or deterministic), identity the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72ba02",
   "metadata": {},
   "source": [
    "## Linear Q-Learning \n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: Discrete or low dimensional continuous \n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - with probability ε => exploration\n",
    "    - with probability 1-ε => exploitaion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff27a11",
   "metadata": {},
   "source": [
    "Linear Q-Learning (with TD) **Q-Update Rule:** $$ q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_a q_t(S_{t+1}, a) - q_t(S_t, A_t) \\right] $$ **Linear Function Approximation:** $$ q(s, a) \\approx \\mathbf{w}^\\top \\phi(s, a) $$ **Weight Update:** $$ \\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_t + \\alpha_t \\, \\delta_t \\, \\nabla_{\\mathbf{w}} q(s, a) $$ Where: $$ \\delta_t = R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a') - q(S_t, A_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f5278",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "- Approach Type: Value-based\n",
    "- Policy Type: Deterministic (ε-greedy)\n",
    "- Observation Space: High dimensional (continuous)\n",
    "- Action Space: Discrete\n",
    "- Explore vs Exploitation:\n",
    "    - uses ε-greedy\n",
    "    - Experience Replay: decorrelates samples\n",
    "    - Target Networks: stabilize Q-value update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040a5a7",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN) **Loss Function:** $$ L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D} \\left[ \\left( r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)^2 \\right] $$ **Target Q-Value:** $$ y = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') $$ **Gradient Update:** $$ \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta) $$ Where: \n",
    "- $\\theta$: parameters of the Q-network  \n",
    "- $\\theta^-$: parameters of the target network (updated periodically)  \n",
    "- $D$: experience replay buffer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971a70c",
   "metadata": {},
   "source": [
    "## Monte Carlo REINFORCE\n",
    "\n",
    "- Approach Type: Policy-based\n",
    "- Policy Type: Stochastic\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "- Explore vs Exploitation:\n",
    "    - Exploration from stochastic policy (softmax or Gaussian)\n",
    "    - No need ε-greedy\n",
    "    - Monte Carlo (update at episode end) => high variance\n",
    "    - Does not bootstrap (no value function), slow learning but avoids bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22137",
   "metadata": {},
   "source": [
    "REINFORCE (Monte Carlo Policy Gradient) **Policy Gradient Update:** $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $$ **Return from timestep \\( t \\):** $$ G_t = \\sum_{k=0}^{T - t - 1} \\gamma^k R_{t + k + 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4e1c0",
   "metadata": {},
   "source": [
    "## Actor-Critic (A2C: Advantage Actor-Critic)\n",
    "\n",
    "- Approach Type: Actor-Critic (Hybrid)\n",
    "- Policy Type: Stochastic (Actor outputs probability distribution over actions)\n",
    "- Observation Space: Discrete or Continuous \n",
    "- Action Space: Discrete or Continuous\n",
    "\n",
    "- Explore vs Exploitation:\n",
    "    - Stochastic policy: Actions are sample from probability distributions \n",
    "\n",
    "- Critic (value function) as a baseline: Reduce variance of policy gradient updates (used to compute Advantage)\n",
    "- Bootstrapping & TD learning\n",
    "- A2C uses TD(0) or n-step return updates\n",
    "- Update directly from gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437b45",
   "metadata": {},
   "source": [
    "A2C (Advantage Actor-Critic)\n",
    "\n",
    "$$\n",
    "L^{\\text{A2C}}(\\theta) = -\\mathbb{E}_t \\left[ \\log \\pi_\\theta(a_t | s_t) \\cdot \\hat{A}_t \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = r_t + \\gamma V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
    "$$\n",
    "\n",
    "or with n-step returns:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = \\left( \\sum_{i=0}^{n-1} \\gamma^i r_{t+i} \\right) + \\gamma^n V_{\\phi}(s_{t+n}) - V_{\\phi}(s_t)\n",
    "$$\n",
    "\n",
    "Critic value loss:\n",
    "\n",
    "$$\n",
    "L^{\\text{VF}}(\\phi) = \\mathbb{E}_t \\left[ \\left( V_\\phi(s_t) - R_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Entropy bonus (to encourage exploration):\n",
    "\n",
    "$$\n",
    "L^{\\text{ENT}}(\\theta) = \\mathbb{E}_t \\left[ \\mathcal{H} \\left[ \\pi_\\theta(\\cdot | s_t) \\right] \\right]\n",
    "$$\n",
    "\n",
    "Total A2C loss:\n",
    "\n",
    "$$\n",
    "L^{\\text{total}} = L^{\\text{A2C}} + c_v L^{\\text{VF}} - c_e L^{\\text{ENT}}\n",
    "$$\n",
    "\n",
    "Where \\( c_v \\) and \\( c_e \\) are the coefficients for the value loss and entropy bonus respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c73a68",
   "metadata": {},
   "source": [
    "### **Comparison Table**\n",
    "|Algorithm|Approach Type|Policy Type|Observation Space|Action Space|Exploration vs Exploitation|\n",
    "|---|---|---|---|---|---|\n",
    "|Linear Q|Value based|Deterministic (ε-greedy)|Discrete|Discrete|ε-greedy|\n",
    "|DQN|Value based|Deterministic (ε-greedy)|Continuous|Discrete|ε-greedy + Replay Buffer + Target Network|\n",
    "|MC REINFORCE|Policy based|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic Policy|\n",
    "|A2C|Actor Critic|Stochastic|Discrete/Continuous|Discrete/Continuous|Stochastic policy + Estimated Advantage + baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f521fb",
   "metadata": {},
   "source": [
    "# **Part 2 : Setting up `Cart-Pole` Agent**\n",
    "\n",
    "1. RL Base Class\n",
    "2. Replay Buffer Class\n",
    "3. Algorithm folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808eaf8",
   "metadata": {},
   "source": [
    "# **Part 3 : Trainning & Playing to stabilize `Cart-Pole` Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f6ee9",
   "metadata": {},
   "source": [
    "# **Part 4 : Evaluate `Cart-Pole` Agent performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3613d40",
   "metadata": {},
   "source": [
    "- Learning efficiency (how well agent learns to recieve higher rewards)\n",
    "- Deployment performance (how well the agent perform in stabilize problem)\n",
    "\n",
    "Analyze and visualize the result to determine: \n",
    "1. Which algo performs best?\n",
    "2. Why does it perform better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c77111",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
