{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRA 503: Deep Reinforment Learning\n",
    "HW 2 Cartpole\n",
    "\n",
    "Napat Aeimwiratchai 65340500020\n",
    "Phattarawat Kadrum 65340500074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1. Understand how a **reinforcement learning agent learns** (i.e., evaluates and improves its policy) in an environment where the true dynamic model is unknown.\n",
    "\n",
    "2. Gain insight into different **reinforcement learning algorithms**, including Monte Carlo methods, the SARSA algorithm, Q-learning, and Double Q-learning. Analyze their strengths and weaknesses.\n",
    "\n",
    "3. Explore approaches to implementing reinforcement learning in **real-world scenarios where the state and action spaces are continuous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1 : Setting up `Cart-Pole` Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. RL Base class**\n",
    "\n",
    "- **Constructor (__init__) to initialize the following parameters:**\n",
    "\n",
    "    - **Control type:** Enumeration of RL algorithms used for decision-making (i.e. Monte Carlo, Temporal Difference, Q-learning, or Double Q-learning).\n",
    "\n",
    "    - **Number of actions:** The total number of discrete actions available to the agent.\n",
    "\n",
    "    - **Action range:** The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "    - **Discretize state weight:** Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "    - **Learning rate:** Determines how quickly the model updates based on new information.\n",
    "\n",
    "    - **Initial epsilon:** The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "    - **Epsilon decay rate:** The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "    - **Final epsilon:** The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "    - **Discount factor:** A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "- **Core Functions**\n",
    "\n",
    "    - **get_discretize_action():** Returns a discrete action based on the current policy.\n",
    "\n",
    "    - **mapping_action():** Converts a discrete action back into a continuous action within the defined action range.\n",
    "\n",
    "    - **discretize_state(): Discretizes the state based on observation weights by divide into period.**\n",
    "\n",
    "    - **decay_epsilon():** Decreases epsilon over time and returns the updated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Algorithm folder**\n",
    "\n",
    "Implement the algorithm\n",
    "\n",
    "- **`Monte Carlo class`**\n",
    "\n",
    "```py\n",
    "     def update(self):\n",
    "        \"\"\"\n",
    "        Update Q-values using reverse-order, first-visit Monte Carlo.\n",
    "        \"\"\"\n",
    "        G = 0  # Initialize return\n",
    "        visited_pairs = set()\n",
    "\n",
    "        # Traverse episode in reverse\n",
    "        for t in reversed(range(len(self.reward_hist))):\n",
    "            state = self.obs_hist[t]\n",
    "            action = self.action_hist[t]\n",
    "            reward = self.reward_hist[t]\n",
    "\n",
    "            G = self.discount_factor * G + reward  # Incremental return\n",
    "\n",
    "            # First-visit MC check\n",
    "            if (state, action) not in visited_pairs:\n",
    "                visited_pairs.add((state, action))\n",
    "                self.n_values[state][action] += 1\n",
    "                alpha = 1 / self.n_values[state][action]  # Incremental mean\n",
    "                self.q_values[state][action] += alpha * (G - self.q_values[state][action])\n",
    "\n",
    "        # Clear episode history\n",
    "        self.obs_hist.clear()\n",
    "        self.action_hist.clear()\n",
    "        self.reward_hist.clear()\n",
    "\n",
    "        # Decay epsilon after episode\n",
    "        self.decay_epsilon()\n",
    "```\n",
    "\n",
    "- **`SARSA class`**\n",
    "\n",
    "```py\n",
    "        def update(self , state, action_idx, reward, next_state, next_action_idx, done):\n",
    "        \"\"\"\n",
    "        Update Q-values using SARSA .\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_values[state][action_idx] += self.lr * (\n",
    "            reward + self.discount_factor * self.q_values[next_state][next_action_idx] - self.q_values[stat][action_idx]\n",
    "        )\n",
    "        \n",
    "        # Move to next state-action pair for the next step in the episode\n",
    "        state = next_state\n",
    "        action_idx = next_action_idx\n",
    "```\n",
    "\n",
    "- **`Q-Learning Class`**\n",
    "\n",
    "```py\n",
    "     def update(self , state, action_idx, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-values using Q-Learning.\n",
    "        \"\"\"\n",
    "        self.q_values[state][action_idx] += self.lr * (\n",
    "            reward + self.discount_factor * np.max(self.q_values[next_state]) - self.q_values[state][action_idx]\n",
    "        )\n",
    "```\n",
    "\n",
    "- **`Double Q-Learning Class1`**\n",
    "\n",
    "```py\n",
    "            \n",
    "    def update(self , state, action_idx, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-values using Double Q-Learning.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            self.qa_values[state][action_idx] += self.lr * (\n",
    "                reward + self.discount_factor * self.qb_values[next_state][np.argmax(self.qa_values[next_state])]\n",
    "                - self.qa_values[state][action_idx]\n",
    "            )\n",
    "        else:\n",
    "            self.qb_values[state][action_idx] += self.lr * (\n",
    "                reward + self.discount_factor * self.qa_values[next_state][np.argmax(self.qb_values[next_state])]\n",
    "                - self.qb_values[state][action_idx]\n",
    "            )\n",
    "        self.q_values[state][action_idx] = self.qa_values[state][action_idx] + self.qb_values[state][action_idx]\n",
    "        \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 : Training & Playing to stabilize `Cart-Pole` Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A. Discrete state and action**\n",
    "\n",
    "For convert continuous to discrete state and action of `cartpole` we need to set `discretize_state_weight` (pose_cart, pose_pole, vel_cart, vel_pole) for divide range of position, velocity to discrete step and `num_of_action` is for divide range of action to discrete step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Experiment Config (State, Action)**  \n",
    " Experiment for testing how number of actions and discretize_state_weights effect to the `cartpole` task.\n",
    "\n",
    "\n",
    " ```py\n",
    " action_of_action = xx  # Number of available discrete action\n",
    " discretize_state_weight = [xx, xx, xx, xx] #  [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **B. Hyperparameter adjusting**\n",
    "\n",
    "For observe how hyperparameter effect to learning trend of `CartPole` agent, we try to adjust of 3 parameters (`Learning Rate`, `Epsilon Decay`, `Discount Factor`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Experiment Config (Hyperparameter)**  \n",
    "This experiment for testing how algorithm's hyperparameter effect to result of `CartPole` task.\n",
    "\n",
    "```py\n",
    "    learning_rate = xx \n",
    "    n_episodes = xx     # Number of episode for Agent's train\n",
    "    start_epsilon = xx  \n",
    "    epsilon_decay = xx  # Reduce the exploration over time\n",
    "    final_epsilon = xx  \n",
    "    discount = xx       # Discount factor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 : Evaluate `Cart-Pole` Agent performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Graph tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate\n",
    "In this experiment we use 2 value of learning rate (0.1 and 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA : \n",
    "$$q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}) - q_t(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "Q_Learning :\n",
    "$$\n",
    "q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q_t(S_{t+1}, a') - q_t(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "Double_Q_Learning :\n",
    "$$\n",
    "q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q'_t(S_{t+1}, a') - q_t(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "q'_{t+1}(S_t, A_t) \\leftarrow q'_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q_t(S_{t+1}, a') - q'_t(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SARSA, Q-Learning, and Double Q-Learning, the learning rate ($\\alpha$) affects how much the model relies on the new Q-value from the recent state during updates.\n",
    "- If the learning rate is too high, the model may become unstable due to high variance in updates.\n",
    "- If the learning rate is too low, the model may converge slowly, but it will be more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *SARSA*\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAlr_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the early stage, the black line (lower learning rate) seem to learn more stably and accumulates higher rewards. But, the blue line (higher learning rate) shows lower rewards. This may be because the high learning rate model tries to converge quickly, but in the early stage, it hasn’t found the optimal policy yet.\n",
    "\n",
    "In the later stage (around episode 3000), the blue line starts to increase rapidly. This could be due to a higher exploitation rate at that point—if the model has found a good policy, the high learning rate allows it to quickly update and reinforce recent Q-values, leading to faster improvement in rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Q_Learning*\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learnlr_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward graph of Q-Learning behaves similarly to the SARSA graph. In the initial episodes, the model with a higher learning rate accumulates less reward. But, after around episode 3000, the model with the higher learning rate has a quick increase in accumulated reward, although it remains somewhat unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Double_Q_Learning*\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learnlr_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Double Q Learning graph, both models seem to have similar characteristics throughout the episodes. This may be because Double Q Learning uses two separate Q-value estimators so it less sensitive to high learning rate, which helps prevent the overestimation of Q-values and reduces the likelihood of overriding past values too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon Decay\n",
    "In this experiment we use 2 value of epsilon decay (0.9988 and 0.9982)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"../images/epsilon_decay.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Monte Carlo*\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MCed.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MCed_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *SARSA*\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAed.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAed_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Q_Learning*\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learned.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learned_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Double_Q_Learning*\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learned.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learned_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Double Q Learning, since it uses two separate policies, a lower epsilon decay (exploiting faster) may cause the model to not explore enough and try to exploit the policies it has. This is evident as the performance line appears to be lower than the one with a higher epsilon decay rate.\n",
    "\n",
    "Graphs from all algorithms (except Monte Carlo) behave similarly. When the epsilon-decay is low (the model exploits faster), the accumulated Q-values are high near the center of the cart and pole positions. However, with a high epsilon-decay rate (the model explores longer), the accumulated Q-values become more uniform, and the graph appears flatter, as can see on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discount factor\n",
    "\n",
    "In this experiment we use 2 value of discount factor (0.1 and 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo :\n",
    "\n",
    "$\\begin{equation}\n",
    "q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{N(S_t,A_t)} (G_t - q(S_t, A_t))\n",
    "\\end{equation}$\n",
    "\n",
    "$\\begin{equation}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t-1}R_T\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factor effect how much future reward value compare to current reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA : \n",
    "$$q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}) - q_t(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factor effect how much future action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_Learning :\n",
    "$$\n",
    "q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q_t(S_{t+1}, a') - q_t(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double_Q_Learning :\n",
    "$$\n",
    "q_{t+1}(S_t, A_t) \\leftarrow q_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q'_t(S_{t+1}, a') - q_t(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "q'_{t+1}(S_t, A_t) \\leftarrow q'_t(S_t, A_t) + \\alpha_t \\left[ R_{t+1} + \\gamma \\max_{a'} q_t(S_{t+1}, a') - q'_t(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q Learning and Double Q Learning dicount factor have similar effect to the value of max $q_t$ but in Double q it may less sensitive for high discount factor value from seperate two q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MCdf_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAdf_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q_Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learndf_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double_Q_Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learndf_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of action\n",
    "In this experiment we use 2 value of number of action (3 and 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"../images/number_action.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "as see from the graph agent have different number of action to choose for interact with environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MCac_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAac_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learnac_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double Q Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learnac_graph.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we didn't see trend of graph that are the significant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete state weight (Pole position)\n",
    "In this experiment we use 2 value of discrete state weight focused on pole position (5 and 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MCst.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSAst.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double_Q_Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/DQ_Learnst.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q_Learning\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q_Learnst.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete state weight is a parameter that indicates how many discrete state ranges when converting from a continuous to a discrete. A higher value means allowing the RL agent to learn more precise by distinguishing subtle differences in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A Q-value is a number that tells the agent how good it is to take a certain action in a given state.\n",
    "\n",
    "$$Q(S, A) = Expected future reward starting from state s and taking action A$$\n",
    "\n",
    "In this case of `CartPole` it mean if the cart is on state S_t and has 3 actions, agent will looking at q table (Q(S_t, A_n) ; for all n) and looking for action that give the highest q_value, then agent will select the best action to response each state.\n",
    "\n",
    "The below graph will show how q_values changing each algorithm when adjusting some hyperparameter\n",
    "- 3D Q_values graph\n",
    "    1. On the vertical axis represent how q_values are in each state\n",
    "    2. On horizonral axis represent Cart position and Pole position \n",
    "        - In the center of both position (cart and pole) represent range of position (min - max) they allow without terminate \n",
    "\n",
    "So, in this graph can visualize how the characteristic trend for each algorithm will be. In the best case graph should shape like a peak of mountain that can show agent can learn to stabilize a pole to upright position and cart position didn't change to much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "    <img src=\"../images/Comparison_table.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image vertical show how hyper-parameter adjust, we use \"Base Case\" as a ground tooth of experiment and adjust parameter to see how it effect to the algorithm. Almost the hyper-parameter in base case are set as high value and in the adjusting part are decrease the parameter's value except the \"Learning rate\" that \"Base Case\" is lower than \"Learning rate\" configuration. \n",
    "\n",
    "**Configuration of \"Base Case\" and adjusting each hyper-parameter**\n",
    "- **Action:** 5 | 7(Base)\n",
    "- **State:** 5 | 11(Base)\n",
    "- **Learning Rate:** 0.1(Base) | 0.5\n",
    "- **Discount Factor:** 0.9982 | 0.9988(Base) \n",
    "- **Epsilon Decay:** 0.1 | 05(Base)\n",
    "\n",
    "From the above comparision table, we can see which state has higher q value and which state has lower value or negative value, which mean higher value(Yellow) cartpole should stabilize better than the lower value(Blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below is represent of the state of the agent (position and velocity of cart and pole)\n",
    "- X axis: Time step\n",
    "- Y axis: Values\n",
    "\n",
    "\n",
    "This table represent label of the graph below \n",
    "\n",
    "|||\n",
    "|--|--|\n",
    "|Position cart|Position pole|\n",
    "|Velocity cart|Velocity pole|\n",
    "\n",
    "Note \n",
    "- Negative value: Left, Positive value: Right\n",
    "- Cart's position range: -3 to 3 \n",
    "- Pole's position range: ~ -0.42 to 0.42 radians\n",
    "\n",
    "Criterion for best model\n",
    "1. Longest alive time\n",
    "2. Has stable cart and pole position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/MC.png\" alt=\"Alt text\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Monte Carlo can alive only 30 steps for all configuration, it mean this algorithm can not do this task (lower reward, lower q value) look at the 3D Q_value graph, we can see the q value graph will be see as negative or zero value, but we will choose the best configuration from the longest alive time, the best configuration is MC with has less action than 'base_case'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/SARSA.png\" alt=\"Alt text\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On SARSA algorithm has alive time more than MC, looking at the pole's position mostly configuration were terminated by pole's position out of bound(more than +-0.4) except 'Learning rate(Purple)' was terminated by cart out of bound(more than +- 3) and we choose the learning rate configuration as the best from alive time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/Q.png\" alt=\"Alt text\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly of configuration on this algorithm agent will slide to left and then try to stabilize and sliding to right side, we can see that on the purple line has the longest alive time, so, we use the 'Learning rate' configuration as the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double Q Learning\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/2Q.png\" alt=\"Alt text\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm look has the same as the above algorithm, so, we choose the 'Learning Rate' configuration for the best config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Best algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align = \"center\">\n",
    "    <img src=\"../images/best.png\" alt=\"Alt text\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we select the best configuration from each algorithm, we try find which algorithm is the best by looking at the highest alive time and how stable of pole position, we can see the orange line has the longest alive time and focus on the pole's position that position of this config has stable osilate for a long time, it also mean it can do their task well but it was terminated at the end of episode, but it still has the longest alive time for all model. \n",
    "\n",
    "So, for our experiment can tell that the best configuration of each algorithm is Q_learning with learning_rate = 0.5.\n",
    "\n",
    "Although this is the best configuration that agent can stabilize but it cannot stay with the same cart's position it has a large amount of sliding, so, it will be the cause of terminate. In future improve, adding other eward term can be better example reward for sliding cart's position that if it has less sliding it will get reward or focus on velocity of pole if pole's velocity has suddenly change it will be penalty term, so, this addition should be improve the agent for stabilize task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
